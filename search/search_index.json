{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"K-NN atau k-nearestneighbor with python Pengertian KNN adalah sebuah metode untuk melakukan klasifikasi terhadap objek berdasarkan data pembelajaran yang jaraknya paling dekat dengan objek tersebut. KNN dapat digunakan untuk masalah prediksi klasifikasi dan regresi. Namun, ini lebih banyak digunakan dalam masalah klasifikasi di industri. Untuk mengevaluasi teknik apa pun kita biasanya melihat 3 aspek penting: Mudah untuk menginterpretasikan output Waktu perhitungan Kekuatan Prediktif Mari kita ambil beberapa contoh untuk menempatkan KNN dalam skala: Algoritma KNN menunjukkan seluruh parameter pertimbangan. Ini biasanya digunakan untuk interpretasi yang mudah dan waktu perhitungan yang rendah. Kita dapat menerapkan model KNN dengan mengikuti langkah-langkah di bawah ini: Muat data Inisialisasi nilai k Untuk mendapatkan kelas yang diprediksi, beralih dari 1 ke jumlah total poin data pelatihan Hitung jarak antara data tes dan setiap baris data pelatihan. Di sini kita akan menggunakan jarak Euclidean sebagai metrik jarak kita karena ini adalah metode yang paling populer. Metrik lain yang dapat digunakan adalah Chebyshev, cosinus, dll. Urutkan jarak yang dihitung dalam urutan naik berdasarkan nilai jarak Dapatkan baris k atas dari array yang diurutkan Dapatkan kelas paling sering dari baris ini Kembalikan kelas yang diprediksi Implementasi dalam Python import pandas as pd import numpy as np import math import operator #### Langkah pertama # Impor data data = pd . read_csv ( iris.csv ) data . head () # Defining a function which calculates euclidean distance between two data points def euclideanDistance ( data1 , data2 , length ): distance = 0 for x in range ( length ): distance += np . square ( data1 [ x ] - data2 [ x ]) return np . sqrt ( distance ) # Defining our KNN model def knn ( trainingSet , testInstance , k ): distances = {} sort = {} length = testInstance . shape [ 1 ] #### Start of STEP 3 # Calculating euclidean distance between each row of training data and test data for x in range ( len ( trainingSet )): #### Start of STEP 3.1 dist = euclideanDistance ( testInstance , trainingSet . iloc [ x ], length ) distances [ x ] = dist [ 0 ] #### End of STEP 3.1 #### Start of STEP 3.2 # Sorting them on the basis of distance sorted_d = sorted ( distances . items (), key = operator . itemgetter ( 1 )) #### End of STEP 3.2 neighbors = [] #### Start of STEP 3.3 # Extracting top k neighbors for x in range ( k ): neighbors . append ( sorted_d [ x ][ 0 ]) #### End of STEP 3.3 classVotes = {} #### Start of STEP 3.4 # Calculating the most freq class in the neighbors for x in range ( len ( neighbors )): response = trainingSet . iloc [ neighbors [ x ]][ - 1 ] if response in classVotes : classVotes [ response ] += 1 else : classVotes [ response ] = 1 #### End of STEP 3.4 #### Start of STEP 3.5 sortedVotes = sorted ( classVotes . items (), key = operator . itemgetter ( 1 ), reverse = True ) return ( sortedVotes [ 0 ][ 0 ], neighbors ) #### End of STEP 3.5 # buat datatest testSet = [[ 7.2 , 3.6 , 5.1 , 2.5 ]] test = pd . DataFrame ( testSet ) #### Start of STEP 2 # Setting number of neighbors = 1 k = 1 #### End of STEP 2 # Running KNN model result , neigh = knn ( data , test , k ) # Predicted class print ( result ) - Iris - virginica # Nearest neighbor print ( neigh ) - [ 141 ] # Setting number of neighbors = 3 k = 3 # Running KNN model result , neigh = knn ( data , test , k ) # Predicted class print ( result ) - Iris - virginica # 3 nearest neighbors print ( neigh ) - [ 141 , 139 , 120 ] # Setting number of neighbors = 5 k = 5 # Running KNN model result , neigh = knn ( data , test , k ) # Predicted class print ( result ) - Iris - virginica # 5 nearest neighbors print ( neigh ) - [ 141 , 139 , 120 , 145 , 144 ] Membandingkan model kita dengan scikit-learn from sklearn.neighbors import KNeighborsClassifier neigh = KNeighborsClassifier ( n_neighbors = 3 ) neigh . fit ( data . iloc [:, 0 : 4 ], data [ Name ]) # Predicted class print ( neigh . predict ( test )) - [ Iris-virginica ] # 3 nearest neighbors print ( neigh . kneighbors ( test )[ 1 ]) - [[ 141 139 120 ]] Kita dapat melihat bahwa kedua model meramalkan kelas yang sama ('Iris-virginica') dan tetangga terdekat yang sama ([141 139 120]). Oleh karena itu kita dapat menyimpulkan bahwa model kami berjalan seperti yang diharapkan.","title":"KNN"},{"location":"#k-nn-atau-k-nearestneighbor-with-python","text":"","title":"K-NN atau\u00a0k-nearestneighbor\u00a0with python"},{"location":"#pengertian","text":"KNN adalah sebuah metode untuk melakukan klasifikasi terhadap objek berdasarkan data pembelajaran yang jaraknya paling dekat dengan objek tersebut. KNN dapat digunakan untuk masalah prediksi klasifikasi dan regresi. Namun, ini lebih banyak digunakan dalam masalah klasifikasi di industri. Untuk mengevaluasi teknik apa pun kita biasanya melihat 3 aspek penting: Mudah untuk menginterpretasikan output Waktu perhitungan Kekuatan Prediktif Mari kita ambil beberapa contoh untuk menempatkan KNN dalam skala: Algoritma KNN menunjukkan seluruh parameter pertimbangan. Ini biasanya digunakan untuk interpretasi yang mudah dan waktu perhitungan yang rendah. Kita dapat menerapkan model KNN dengan mengikuti langkah-langkah di bawah ini: Muat data Inisialisasi nilai k Untuk mendapatkan kelas yang diprediksi, beralih dari 1 ke jumlah total poin data pelatihan Hitung jarak antara data tes dan setiap baris data pelatihan. Di sini kita akan menggunakan jarak Euclidean sebagai metrik jarak kita karena ini adalah metode yang paling populer. Metrik lain yang dapat digunakan adalah Chebyshev, cosinus, dll. Urutkan jarak yang dihitung dalam urutan naik berdasarkan nilai jarak Dapatkan baris k atas dari array yang diurutkan Dapatkan kelas paling sering dari baris ini Kembalikan kelas yang diprediksi","title":"Pengertian"},{"location":"#implementasi-dalam-python","text":"import pandas as pd import numpy as np import math import operator #### Langkah pertama # Impor data data = pd . read_csv ( iris.csv ) data . head () # Defining a function which calculates euclidean distance between two data points def euclideanDistance ( data1 , data2 , length ): distance = 0 for x in range ( length ): distance += np . square ( data1 [ x ] - data2 [ x ]) return np . sqrt ( distance ) # Defining our KNN model def knn ( trainingSet , testInstance , k ): distances = {} sort = {} length = testInstance . shape [ 1 ] #### Start of STEP 3 # Calculating euclidean distance between each row of training data and test data for x in range ( len ( trainingSet )): #### Start of STEP 3.1 dist = euclideanDistance ( testInstance , trainingSet . iloc [ x ], length ) distances [ x ] = dist [ 0 ] #### End of STEP 3.1 #### Start of STEP 3.2 # Sorting them on the basis of distance sorted_d = sorted ( distances . items (), key = operator . itemgetter ( 1 )) #### End of STEP 3.2 neighbors = [] #### Start of STEP 3.3 # Extracting top k neighbors for x in range ( k ): neighbors . append ( sorted_d [ x ][ 0 ]) #### End of STEP 3.3 classVotes = {} #### Start of STEP 3.4 # Calculating the most freq class in the neighbors for x in range ( len ( neighbors )): response = trainingSet . iloc [ neighbors [ x ]][ - 1 ] if response in classVotes : classVotes [ response ] += 1 else : classVotes [ response ] = 1 #### End of STEP 3.4 #### Start of STEP 3.5 sortedVotes = sorted ( classVotes . items (), key = operator . itemgetter ( 1 ), reverse = True ) return ( sortedVotes [ 0 ][ 0 ], neighbors ) #### End of STEP 3.5 # buat datatest testSet = [[ 7.2 , 3.6 , 5.1 , 2.5 ]] test = pd . DataFrame ( testSet ) #### Start of STEP 2 # Setting number of neighbors = 1 k = 1 #### End of STEP 2 # Running KNN model result , neigh = knn ( data , test , k ) # Predicted class print ( result ) - Iris - virginica # Nearest neighbor print ( neigh ) - [ 141 ] # Setting number of neighbors = 3 k = 3 # Running KNN model result , neigh = knn ( data , test , k ) # Predicted class print ( result ) - Iris - virginica # 3 nearest neighbors print ( neigh ) - [ 141 , 139 , 120 ] # Setting number of neighbors = 5 k = 5 # Running KNN model result , neigh = knn ( data , test , k ) # Predicted class print ( result ) - Iris - virginica # 5 nearest neighbors print ( neigh ) - [ 141 , 139 , 120 , 145 , 144 ]","title":"Implementasi dalam Python"},{"location":"#membandingkan-model-kita-dengan-scikit-learn","text":"from sklearn.neighbors import KNeighborsClassifier neigh = KNeighborsClassifier ( n_neighbors = 3 ) neigh . fit ( data . iloc [:, 0 : 4 ], data [ Name ]) # Predicted class print ( neigh . predict ( test )) - [ Iris-virginica ] # 3 nearest neighbors print ( neigh . kneighbors ( test )[ 1 ]) - [[ 141 139 120 ]] Kita dapat melihat bahwa kedua model meramalkan kelas yang sama ('Iris-virginica') dan tetangga terdekat yang sama ([141 139 120]). Oleh karena itu kita dapat menyimpulkan bahwa model kami berjalan seperti yang diharapkan.","title":"Membandingkan model kita dengan scikit-learn"},{"location":"dst/","text":"Decision Tree with python Pengertian Decision Tree (Pohon Keputusan) adalah pohon dimana setiap cabangnyamenunjukkan pilihan diantara sejumlah alternatif pilihan yang ada, dan setiapdaunnya menunjukkan keputusan yang dipilih.Decision tree biasa digunakan untuk mendapatkan informasi untuk tujuanpengambilan sebuah keputusan. Decision tree dimulai dengan sebuah root node(titik awal) yang dipakai oleh user untuk mengambil tindakan. Dari node root ini,user memecahnya sesuai dengan algoritma decision tree. Hasil akhirnya adalahsebuah decision tree dengan setiap cabangnya menunjukkan kemungkinan sekenario dari keputusan yang diambil serta hasilnya . Contoh Pemanfaatan Decision Tree Diagnosa beberapa penyakit seperti kanker, hipertensi, stroke. Menentukan apakah dengan kondisi yang ada layak untuk bermaintenis atau tidak Menentukan apakah sebuah investasi bisnis layak dilakukan atau tidak Pemilihan pegawai teladan sesuai dengan kriteria tertentu Deteksi gangguan pada komputer atau jaringan komputer Pemilihan produk seperti rumah, kendaraan dan lain lain Kelebihan dan Kekurangan Decision Tree Classification \u00b6 Kelebihan: \u00b6 Daerah pengambilan keputusan yang sebelumnya kompleks dan sangat global, dapat diubah menjadi lebih simpel dan spesifik. Eliminasi perhitungan-perhitungan yang tidak diperlukan, karena ketika menggunakan metode pohon keputusan maka sample diuji hanya berdasarkan kriteria atau kelas tertentu. Fleksibel untuk memilih fitur dari internal node yang berbeda, fitur yang terpilih akan membedakan suatu kriteria dibandingkan kriteria yang lain dalam node yang sama. Kefleksibelan metode pohon keputusan ini meningkatkan kualitas keputusan yang dihasilkan jika dibandingkan ketika menggunakan metode penghitungan satu tahap yang lebih konvensional Dalam analisis multivariat, dengan kriteria dan kelas yang jumlahnya sangat banyak, seorang penguji biasanya perlu untuk mengestimasikan baik itu distribusi dimensi tinggi ataupun parameter tertentu dari distribusi kelas tersebut. Metode pohon keputusan dapat menghindari munculnya permasalahan ini dengan menggunakan criteria yang jumlahnya lebih sedikit pada setiap node internal tanpa banyak mengurangi kualitas keputusan yang dihasilkan. Kekurangan: \u00b6 Terjadi overlap terutama ketika kelas-kelas dan criteria yang digunakan jumlahnya sangat banyak. Hal tersebut juga dapat menyebabkan meningkatnya waktu pengambilan keputusan dan jumlah memori yang diperlukan. Pengakumulasian jumlah eror dari setiap tingkat dalam sebuah pohon keputusan yang besar. Kesulitan dalam mendesain pohon keputusan yang optimal. Hasil kualitas keputusan yang didapatkan dari metode pohon keputusan sangat tergantung pada bagaimana pohon tersebut didesain. Algoritma Decision Tree Classification \u00b6 ID3 Gini Index Chi-Square Reduction in Variance Namun disini kita hanya membahas ID3 dan Gini index saja. Untuk mendapatkan nilai Information Gain dan Gain Ratio, terlebih dahulu kita harus menghitung nilai entropy. Eentropy digunakan untuk mengukur nilai ketidak murnian sekumpulan objek pada setiap cabang pada suatu atribut. Mila(2015) menyatakan rumus Entropy terdapat pada persamaan : Algoritma ID3 menggunakan entropi untuk menghitung homogenitas sampel. Jika sampel benar-benar homogen, entropinya nol dan jika sampel dibagi sama rata, maka entropinya satu. Kami akan menggunakan implementasi yang disediakan oleh kerangka pembelajaran mesin python yang dikenal sebagai scikit-belajar untuk memahami Pohon Keputusan. download data Implementasi dalam Python import os import numpy as np import pandas as pd import numpy as np , pandas as pd import matplotlib.pyplot as plt from sklearn import tree , metrics #### Langkah pertama # Impor data data = pd . read_csv ( data/car_quality/car.data , names = [ buying , maint , doors , persons , lug_boot , safety , class ]) data . head () Identifikasi target variabel data [ class ], class_names = pd . factorize ( data [ class ]) Identifikasi variabel prediktor dan encode variabel string apa pun ke kode integer yang setara data [ buying ], _ = pd . factorize ( data [ buying ]) data [ maint ], _ = pd . factorize ( data [ maint ]) data [ doors ], _ = pd . factorize ( data [ doors ]) data [ persons ], _ = pd . factorize ( data [ persons ]) data [ lug_boot ], _ = pd . factorize ( data [ lug_boot ]) data [ safety ], _ = pd . factorize ( data [ safety ]) data . head () Pilih fitur prediktsi dan variabel target X = data . iloc [:,: - 1 ] y = data . iloc [:, - 1 ] Train test split: X_train , X_test , y_train , y_test = model_selection . train_test_split ( X , y , test_size = 0.3 , random_state = 0 ) Training/model fitting dtree = tree.DecisionTreeClassifier(criterion= entropy , max_depth=3, random_state=0) dtree.fit(X_train, y_train) Studi parameter model: # use the model to make predictions with the test data y_pred = dtree.predict(X_test) # how did our model perform? count_misclassified = (y_test != y_pred).sum() print( Misclassified samples: {} .format(count_misclassified)) accuracy = metrics.accuracy_score(y_test, y_pred) print( Accuracy: {:.2f} .format(accuracy)) Visualization of the decision graph: import graphviz feature_names = X.columns dot_data = tree.export_graphviz(dtree, out_file=None, filled=True, rounded=True, feature_names=feature_names, class_names=class_names) graph = graphviz.Source(dot_data) graph Refrensi https://acadgild.com/blog/decision-tree-python-code http://newbiegameku.blogspot.com/2014/07/pengertian-decision-tree.html http://eprints.ums.ac.id/47859/1/naskah%20publikasi%20revisi.pdf","title":"Decision Tree"},{"location":"dst/#decision-tree-with-python","text":"","title":"Decision Tree with python"},{"location":"dst/#pengertian","text":"Decision Tree (Pohon Keputusan) adalah pohon dimana setiap cabangnyamenunjukkan pilihan diantara sejumlah alternatif pilihan yang ada, dan setiapdaunnya menunjukkan keputusan yang dipilih.Decision tree biasa digunakan untuk mendapatkan informasi untuk tujuanpengambilan sebuah keputusan. Decision tree dimulai dengan sebuah root node(titik awal) yang dipakai oleh user untuk mengambil tindakan. Dari node root ini,user memecahnya sesuai dengan algoritma decision tree. Hasil akhirnya adalahsebuah decision tree dengan setiap cabangnya menunjukkan kemungkinan sekenario dari keputusan yang diambil serta hasilnya .","title":"Pengertian"},{"location":"dst/#contoh-pemanfaatan-decision-tree","text":"Diagnosa beberapa penyakit seperti kanker, hipertensi, stroke. Menentukan apakah dengan kondisi yang ada layak untuk bermaintenis atau tidak Menentukan apakah sebuah investasi bisnis layak dilakukan atau tidak Pemilihan pegawai teladan sesuai dengan kriteria tertentu Deteksi gangguan pada komputer atau jaringan komputer Pemilihan produk seperti rumah, kendaraan dan lain lain","title":"Contoh Pemanfaatan Decision Tree"},{"location":"dst/#kelebihan-dan-kekurangan-decision-tree-classification","text":"","title":"Kelebihan dan Kekurangan Decision Tree Classification\u00b6"},{"location":"dst/#kelebihan","text":"Daerah pengambilan keputusan yang sebelumnya kompleks dan sangat global, dapat diubah menjadi lebih simpel dan spesifik. Eliminasi perhitungan-perhitungan yang tidak diperlukan, karena ketika menggunakan metode pohon keputusan maka sample diuji hanya berdasarkan kriteria atau kelas tertentu. Fleksibel untuk memilih fitur dari internal node yang berbeda, fitur yang terpilih akan membedakan suatu kriteria dibandingkan kriteria yang lain dalam node yang sama. Kefleksibelan metode pohon keputusan ini meningkatkan kualitas keputusan yang dihasilkan jika dibandingkan ketika menggunakan metode penghitungan satu tahap yang lebih konvensional Dalam analisis multivariat, dengan kriteria dan kelas yang jumlahnya sangat banyak, seorang penguji biasanya perlu untuk mengestimasikan baik itu distribusi dimensi tinggi ataupun parameter tertentu dari distribusi kelas tersebut. Metode pohon keputusan dapat menghindari munculnya permasalahan ini dengan menggunakan criteria yang jumlahnya lebih sedikit pada setiap node internal tanpa banyak mengurangi kualitas keputusan yang dihasilkan.","title":"Kelebihan:\u00b6"},{"location":"dst/#kekurangan","text":"Terjadi overlap terutama ketika kelas-kelas dan criteria yang digunakan jumlahnya sangat banyak. Hal tersebut juga dapat menyebabkan meningkatnya waktu pengambilan keputusan dan jumlah memori yang diperlukan. Pengakumulasian jumlah eror dari setiap tingkat dalam sebuah pohon keputusan yang besar. Kesulitan dalam mendesain pohon keputusan yang optimal. Hasil kualitas keputusan yang didapatkan dari metode pohon keputusan sangat tergantung pada bagaimana pohon tersebut didesain.","title":"Kekurangan:\u00b6"},{"location":"dst/#algoritma-decision-tree-classification","text":"ID3 Gini Index Chi-Square Reduction in Variance Namun disini kita hanya membahas ID3 dan Gini index saja. Untuk mendapatkan nilai Information Gain dan Gain Ratio, terlebih dahulu kita harus menghitung nilai entropy. Eentropy digunakan untuk mengukur nilai ketidak murnian sekumpulan objek pada setiap cabang pada suatu atribut. Mila(2015) menyatakan rumus Entropy terdapat pada persamaan : Algoritma ID3 menggunakan entropi untuk menghitung homogenitas sampel. Jika sampel benar-benar homogen, entropinya nol dan jika sampel dibagi sama rata, maka entropinya satu. Kami akan menggunakan implementasi yang disediakan oleh kerangka pembelajaran mesin python yang dikenal sebagai scikit-belajar untuk memahami Pohon Keputusan. download data","title":"Algoritma Decision Tree Classification\u00b6"},{"location":"dst/#implementasi-dalam-python","text":"import os import numpy as np import pandas as pd import numpy as np , pandas as pd import matplotlib.pyplot as plt from sklearn import tree , metrics #### Langkah pertama # Impor data data = pd . read_csv ( data/car_quality/car.data , names = [ buying , maint , doors , persons , lug_boot , safety , class ]) data . head ()","title":"Implementasi dalam Python"},{"location":"dst/#identifikasi-target-variabel","text":"data [ class ], class_names = pd . factorize ( data [ class ])","title":"Identifikasi target variabel"},{"location":"dst/#identifikasi-variabel-prediktor-dan-encode-variabel-string-apa-pun-ke-kode-integer-yang-setara","text":"data [ buying ], _ = pd . factorize ( data [ buying ]) data [ maint ], _ = pd . factorize ( data [ maint ]) data [ doors ], _ = pd . factorize ( data [ doors ]) data [ persons ], _ = pd . factorize ( data [ persons ]) data [ lug_boot ], _ = pd . factorize ( data [ lug_boot ]) data [ safety ], _ = pd . factorize ( data [ safety ]) data . head ()","title":"Identifikasi variabel prediktor dan encode variabel string apa pun ke kode integer yang setara"},{"location":"dst/#pilih-fitur-prediktsi-dan-variabel-target","text":"X = data . iloc [:,: - 1 ] y = data . iloc [:, - 1 ]","title":"Pilih fitur prediktsi dan variabel target"},{"location":"dst/#train-test-split","text":"X_train , X_test , y_train , y_test = model_selection . train_test_split ( X , y , test_size = 0.3 , random_state = 0 )","title":"Train test split:"},{"location":"dst/#trainingmodel-fitting","text":"dtree = tree.DecisionTreeClassifier(criterion= entropy , max_depth=3, random_state=0) dtree.fit(X_train, y_train)","title":"Training/model fitting"},{"location":"dst/#studi-parameter-model","text":"# use the model to make predictions with the test data y_pred = dtree.predict(X_test) # how did our model perform? count_misclassified = (y_test != y_pred).sum() print( Misclassified samples: {} .format(count_misclassified)) accuracy = metrics.accuracy_score(y_test, y_pred) print( Accuracy: {:.2f} .format(accuracy))","title":"Studi parameter model:"},{"location":"dst/#visualization-of-the-decision-graph","text":"import graphviz feature_names = X.columns dot_data = tree.export_graphviz(dtree, out_file=None, filled=True, rounded=True, feature_names=feature_names, class_names=class_names) graph = graphviz.Source(dot_data) graph","title":"Visualization of the decision graph:"},{"location":"dst/#refrensi","text":"https://acadgild.com/blog/decision-tree-python-code http://newbiegameku.blogspot.com/2014/07/pengertian-decision-tree.html http://eprints.ums.ac.id/47859/1/naskah%20publikasi%20revisi.pdf","title":"Refrensi"}]}